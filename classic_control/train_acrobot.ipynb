{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from a2c import A2CAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create Gym environment\n",
    "a2c_env = \"Acrobot-v1\"\n",
    "env = gym.make(a2c_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if a2c_env == \"Acrobot-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 7e-4\n",
    "\n",
    "agent = A2CAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 300\n",
    "max_steps = 500\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory.append((state, action, reward, next_state, done))\n",
    "        episode_reward += reward  \n",
    "        if done or step == max_steps:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n",
    "            break\n",
    "        state = next_state\n",
    "    agent.update(trajectory, 0)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "a2c_rewards = episode_rewards\n",
    "a2c_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/a2c/' + a2c_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((a2c_runtime, a2c_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DR TRPO Agent (KL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from a2c_dr_trpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create Gym environment\n",
    "kl_env = \"Acrobot-v1\"\n",
    "env = gym.make(kl_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if kl_env == \"Acrobot-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 5e-3\n",
    "    beta = 0.5\n",
    "\n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "############################### MC Updates  (Full Episode) ###############################\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 300\n",
    "max_steps = 500\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    first_state = env.reset()\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "        avg_episode_reward = episode_reward/env.action_space.n\n",
    "    \n",
    "    # add randomness for better exploration\n",
    "    beta += np.random.random()*0.1\n",
    "    if(state_adv[0] == state_adv[1]) and (state_adv[1] == state_adv[2]) and avg_episode_reward  <= -490:\n",
    "        state_adv[0] += (np.random.random()-0.5)*2\n",
    "        state_adv[1] += (np.random.random()-0.5)*2\n",
    "        state_adv[2] += (np.random.random()-0.5)*2\n",
    "\n",
    "    policy_loss = agent.compute_policy_loss_kl(state, state_adv, beta)\n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_kl_rewards = episode_rewards\n",
    "dr_trpo_kl_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_kl/' + kl_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_kl_runtime, dr_trpo_kl_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DR TRPO Agent (Wasserstein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from a2c_dr_trpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "wass_env = \"Acrobot-v1\"\n",
    "# Create Gym environment\n",
    "env = gym.make(wass_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if wass_env == \"Acrobot-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 5e-3\n",
    "    \n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "############################### MC Updates  (Full Episode) ###############################\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 300\n",
    "max_steps = 500\n",
    "total_adv_diff = 0\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    first_state = env.reset()\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "        avg_episode_reward = episode_reward/env.action_space.n\n",
    "    \n",
    "    # add randomness for better exploration in the beginning\n",
    "    if(state_adv[0] == state_adv[1]) and (state_adv[1] == state_adv[2]) and avg_episode_reward < -490:\n",
    "        state_adv[0] += (np.random.random()-0.5)*2\n",
    "        state_adv[1] += (np.random.random()-0.5)*2\n",
    "        state_adv[2] += (np.random.random()-0.5)*2\n",
    "    total_adv_diff += max(abs(state_adv[1] - state_adv[0]), abs(state_adv[2] - state_adv[0]), abs(state_adv[2] - state_adv[1]))\n",
    "    beta = total_adv_diff/episode\n",
    "    beta = beta + np.random.random()*0.05\n",
    "    # print(beta)\n",
    "    policy_loss = agent.compute_policy_loss_wass(state, state_adv, beta)\n",
    "\n",
    "    \n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_wass_rewards = episode_rewards\n",
    "dr_trpo_wass_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_wass/' + wass_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_wass_runtime, dr_trpo_wass_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
